{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bdbefd7",
   "metadata": {},
   "source": [
    "#  Enhanced Age Group Classification Notebook\n",
    "This notebook breaks down `training.py` into parts with explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1193e27a",
   "metadata": {},
   "source": [
    "###  1. Robust Data Loading\n",
    "This section attempts to load training and test datasets from various possible file names. It ensures that if one naming convention fails, others are tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4ab142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ROBUST DATA LOADING with multiple filename attempts\n",
    "def load_data_robust():\n",
    "    file_combinations = [\n",
    "        ('train.csv', 'test.csv'),\n",
    "        ('Train_Data.csv', 'Test_Data.csv'),\n",
    "        ('training_data.csv', 'testing_data.csv')\n",
    "    ]\n",
    "    \n",
    "    for train_file, test_file in file_combinations:\n",
    "        try:\n",
    "            train_df = pd.read_csv(train_file)\n",
    "            test_df = pd.read_csv(test_file)\n",
    "            print(f\" Data loaded: {train_file}, {test_file}\")\n",
    "            print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "            return train_df, test_df\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "    \n",
    "    raise Exception(\" Could not find data files. Please check filenames.\")\n",
    "\n",
    "train_df, test_df = load_data_robust()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797883b8",
   "metadata": {},
   "source": [
    "###  2. Advanced Data Preprocessing\n",
    "This part handles cleaning of the target column `age_group`, including encoding it and dropping missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45184277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ADVANCED DATA PREPROCESSING\n",
    "print(\"\\n DATA ANALYSIS & PREPROCESSING\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Handle target variable encoding\n",
    "if 'age_group' in train_df.columns:\n",
    "    initial_rows = len(train_df)\n",
    "    train_df = train_df.dropna(subset=['age_group'])\n",
    "    print(f\"Dropped {initial_rows - len(train_df)} rows with missing target\")\n",
    "    \n",
    "    # Robust target encoding\n",
    "    if train_df['age_group'].dtype == 'object':\n",
    "        unique_vals = train_df['age_group'].unique()\n",
    "        print(f\"Target values found: {unique_vals}\")\n",
    "        \n",
    "        # Multiple encoding strategies\n",
    "        if 'Adult' in unique_vals and 'Senior' in unique_vals:\n",
    "            train_df['age_group'] = train_df['age_group'].map({'Adult': 0, 'Senior': 1})\n",
    "        elif 'adult' in unique_vals and 'senior' in unique_vals:\n",
    "            train_df['age_group'] = train_df['age_group'].map({'adult': 0, 'senior': 1})\n",
    "        else:\n",
    "            # Try numeric conversion\n",
    "            train_df['age_group'] = pd.to_numeric(train_df['age_group'], errors='coerce')\n",
    "    \n",
    "    # Ensure binary encoding\n",
    "    train_df['age_group'] = train_df['age_group'].astype(int)\n",
    "    target_dist = train_df['age_group'].value_counts(normalize=True)\n",
    "    print(f\"Target distribution: Adult(0): {target_dist.get(0, 0):.3f}, Senior(1): {target_dist.get(1, 0):.3f}\")\n",
    "\n",
    "# Define features\n",
    "TARGET = 'age_group'\n",
    "ID_COL = 'SEQN' if 'SEQN' in train_df.columns else None\n",
    "exclude_cols = [TARGET] + ([ID_COL] if ID_COL else [])\n",
    "features = [c for c in train_df.columns if c not in exclude_cols]\n",
    "\n",
    "print(f\"Features to use: {features}\")\n",
    "print(f\"Number of features: {len(features)}\")\n",
    "\n",
    "X = train_df[features].copy()\n",
    "y = train_df[TARGET].copy()\n",
    "X_test = test_df[features].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede916e",
   "metadata": {},
   "source": [
    "###  3. Domain-Specific Feature Engineering\n",
    "Adds medically-informed and interaction-based features that may help predict the `age_group` effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feceb09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. DOMAIN-SPECIFIC FEATURE ENGINEERING\n",
    "def create_medical_features(df):\n",
    "    \"\"\"Create medically-informed features for age prediction\"\"\"\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Missing value indicators (important for medical data)\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            df_new[f'{col}_is_missing'] = df[col].isnull().astype(int)\n",
    "    \n",
    "    # Gender-specific features\n",
    "    if 'RIAGENDR' in df.columns:\n",
    "        df_new['is_male'] = (df['RIAGENDR'] == 1).astype(int)\n",
    "        df_new['is_female'] = (df['RIAGENDR'] == 2).astype(int)\n",
    "    \n",
    "    # BMI-based health indicators (crucial for age prediction)\n",
    "    if 'BMXBMI' in df.columns:\n",
    "        bmi = df['BMXBMI'].fillna(df['BMXBMI'].median())\n",
    "        \n",
    "        # Clinical BMI categories\n",
    "        df_new['bmi_underweight'] = (bmi < 18.5).astype(int)\n",
    "        df_new['bmi_normal'] = ((bmi >= 18.5) & (bmi < 25)).astype(int) \n",
    "        df_new['bmi_overweight'] = ((bmi >= 25) & (bmi < 30)).astype(int)\n",
    "        df_new['bmi_obese_1'] = ((bmi >= 30) & (bmi < 35)).astype(int)\n",
    "        df_new['bmi_obese_2'] = (bmi >= 35).astype(int)\n",
    "        \n",
    "        # Age-related BMI patterns\n",
    "        df_new['bmi_senior_risk'] = ((bmi < 20) | (bmi > 35)).astype(int)\n",
    "        df_new['bmi_optimal_range'] = ((bmi >= 20) & (bmi <= 27)).astype(int)\n",
    "        \n",
    "        # BMI transformations\n",
    "        df_new['bmi_squared'] = bmi ** 2\n",
    "        df_new['bmi_log'] = np.log1p(bmi)\n",
    "        df_new['bmi_reciprocal'] = 1 / (bmi + 1)\n",
    "    \n",
    "    # Glucose metabolism (key age predictor)\n",
    "    if 'LBXGLU' in df.columns:\n",
    "        glucose = df['LBXGLU'].fillna(df['LBXGLU'].median())\n",
    "        \n",
    "        # Clinical glucose thresholds\n",
    "        df_new['glucose_hypoglycemic'] = (glucose < 70).astype(int)\n",
    "        df_new['glucose_normal'] = ((glucose >= 70) & (glucose < 100)).astype(int)\n",
    "        df_new['glucose_prediabetic'] = ((glucose >= 100) & (glucose < 126)).astype(int)\n",
    "        df_new['glucose_diabetic'] = (glucose >= 126).astype(int)\n",
    "        df_new['glucose_severely_high'] = (glucose >= 200).astype(int)\n",
    "        \n",
    "        # Age-related glucose patterns\n",
    "        df_new['glucose_age_risk'] = np.where(glucose >= 126, 3,\n",
    "                                     np.where(glucose >= 100, 2,\n",
    "                                     np.where(glucose < 70, 1, 0)))\n",
    "        \n",
    "        # Glucose transformations\n",
    "        df_new['glucose_log'] = np.log1p(glucose)\n",
    "        df_new['glucose_sqrt'] = np.sqrt(glucose)\n",
    "    \n",
    "    # Glucose tolerance test (OGTT)\n",
    "    if 'LBXGLT' in df.columns:\n",
    "        glt = df['LBXGLT'].fillna(df['LBXGLT'].median())\n",
    "        df_new['glt_normal'] = (glt < 140).astype(int)\n",
    "        df_new['glt_impaired'] = ((glt >= 140) & (glt < 200)).astype(int)\n",
    "        df_new['glt_diabetic'] = (glt >= 200).astype(int)\n",
    "        df_new['glt_log'] = np.log1p(glt)\n",
    "    \n",
    "    # Insulin levels (metabolic health)\n",
    "    if 'LBXIN' in df.columns:\n",
    "        insulin = df['LBXIN'].fillna(df['LBXIN'].median())\n",
    "        df_new['insulin_low'] = (insulin < 5).astype(int)\n",
    "        df_new['insulin_normal'] = ((insulin >= 5) & (insulin < 25)).astype(int)\n",
    "        df_new['insulin_high'] = ((insulin >= 25) & (insulin < 50)).astype(int)\n",
    "        df_new['insulin_very_high'] = (insulin >= 50).astype(int)\n",
    "        df_new['insulin_log'] = np.log1p(insulin)\n",
    "    \n",
    "    # Physical activity (lifestyle factor)\n",
    "    if 'PAQ605' in df.columns:\n",
    "        df_new['is_active'] = (df['PAQ605'] == 1).astype(int)\n",
    "        df_new['is_inactive'] = (df['PAQ605'] == 2).astype(int)\n",
    "    \n",
    "    # Diabetes diagnosis\n",
    "    if 'DIQ010' in df.columns:\n",
    "        df_new['has_diabetes'] = (df['DIQ010'] == 1).astype(int)\n",
    "        df_new['no_diabetes'] = (df['DIQ010'] == 2).astype(int)\n",
    "    \n",
    "    # CRITICAL INTERACTION FEATURES (often the key to high performance)\n",
    "    if 'BMXBMI' in df.columns and 'LBXGLU' in df.columns:\n",
    "        bmi = df['BMXBMI'].fillna(df['BMXBMI'].median())\n",
    "        glucose = df['LBXGLU'].fillna(df['LBXGLU'].median())\n",
    "        \n",
    "        df_new['bmi_glucose_product'] = bmi * glucose\n",
    "        df_new['bmi_glucose_ratio'] = bmi / (glucose + 1)\n",
    "        df_new['glucose_bmi_ratio'] = glucose / (bmi + 1)\n",
    "        df_new['metabolic_syndrome_risk'] = ((bmi > 30) & (glucose > 100)).astype(int)\n",
    "    \n",
    "    if 'LBXGLU' in df.columns and 'LBXIN' in df.columns:\n",
    "        glucose = df['LBXGLU'].fillna(df['LBXGLU'].median())\n",
    "        insulin = df['LBXIN'].fillna(df['LBXIN'].median())\n",
    "        \n",
    "        df_new['glucose_insulin_ratio'] = glucose / (insulin + 1)\n",
    "        df_new['insulin_resistance_index'] = glucose * insulin\n",
    "        df_new['homa_ir_approx'] = (glucose * insulin) / 405  # Simplified HOMA-IR\n",
    "    \n",
    "    if 'RIAGENDR' in df.columns and 'BMXBMI' in df.columns:\n",
    "        df_new['male_high_bmi'] = ((df['RIAGENDR'] == 1) & (df['BMXBMI'] > 30)).astype(int)\n",
    "        df_new['female_low_bmi'] = ((df['RIAGENDR'] == 2) & (df['BMXBMI'] < 20)).astype(int)\n",
    "    \n",
    "    # Comprehensive health risk score\n",
    "    risk_score = 0\n",
    "    if 'DIQ010' in df.columns:\n",
    "        risk_score += (df['DIQ010'] == 1).astype(int) * 3  # Diabetes major risk\n",
    "    if 'BMXBMI' in df.columns:\n",
    "        risk_score += (df['BMXBMI'] > 35).astype(int) * 2  # Severe obesity\n",
    "        risk_score += ((df['BMXBMI'] > 30) & (df['BMXBMI'] <= 35)).astype(int) * 1\n",
    "    if 'LBXGLU' in df.columns:\n",
    "        risk_score += (df['LBXGLU'] > 126).astype(int) * 2  # Diabetes range\n",
    "        risk_score += ((df['LBXGLU'] > 100) & (df['LBXGLU'] <= 126)).astype(int) * 1\n",
    "    if 'PAQ605' in df.columns:\n",
    "        risk_score += (df['PAQ605'] == 2).astype(int) * 1  # Inactivity\n",
    "    \n",
    "    df_new['total_health_risk'] = risk_score\n",
    "    df_new['high_risk_senior'] = (risk_score >= 4).astype(int)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "# Apply enhanced feature engineering\n",
    "print(\"\\nðŸ”§ FEATURE ENGINEERING\")\n",
    "print(\"-\" * 30)\n",
    "X_enhanced = create_medical_features(X)\n",
    "X_test_enhanced = create_medical_features(X_test)\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "print(f\"Enhanced features: {X_enhanced.shape[1]}\")\n",
    "print(f\"Added features: {X_enhanced.shape[1] - X.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a835e965",
   "metadata": {},
   "source": [
    "###  4. Advanced Missing Value Handling\n",
    "Uses Iterative Imputation, KNN, and Simple strategies to fill in missing data in a smart way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc411623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. ADVANCED MISSING VALUE HANDLING\n",
    "def advanced_imputation(X_train, X_test):\n",
    "    \"\"\"Multiple imputation strategies\"\"\"\n",
    "    \n",
    "    # Identify feature types\n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Strategy 1: Iterative imputation (most sophisticated)\n",
    "    iterative_imputer = IterativeImputer(\n",
    "        estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "        max_iter=10,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Strategy 2: KNN imputation\n",
    "    knn_imputer = KNNImputer(n_neighbors=5)\n",
    "    \n",
    "    # Strategy 3: Median/Mode imputation\n",
    "    simple_imputer = SimpleImputer(strategy='median')\n",
    "    \n",
    "    # Use iterative for most features, KNN for backup\n",
    "    X_train_imputed = pd.DataFrame(\n",
    "        iterative_imputer.fit_transform(X_train[numeric_features]),\n",
    "        columns=numeric_features,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    X_test_imputed = pd.DataFrame(\n",
    "        iterative_imputer.transform(X_test[numeric_features]),\n",
    "        columns=numeric_features,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    # Add non-numeric columns back\n",
    "    for col in X_train.columns:\n",
    "        if col not in numeric_features:\n",
    "            X_train_imputed[col] = X_train[col].fillna(X_train[col].mode()[0] if not X_train[col].mode().empty else 0)\n",
    "            X_test_imputed[col] = X_test[col].fillna(X_train[col].mode()[0] if not X_train[col].mode().empty else 0)\n",
    "    \n",
    "    return X_train_imputed, X_test_imputed\n",
    "\n",
    "X_enhanced, X_test_enhanced = advanced_imputation(X_enhanced, X_test_enhanced)\n",
    "print(\" Advanced imputation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730743f",
   "metadata": {},
   "source": [
    "###  5. Feature Selection\n",
    "Selects the top features using statistical tests, mutual information, and random forest importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b658b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. FEATURE SELECTION with multiple methods\n",
    "def intelligent_feature_selection(X, y, k=None):\n",
    "    \"\"\"Multi-method feature selection\"\"\"\n",
    "    \n",
    "    if k is None:\n",
    "        k = min(50, X.shape[1])  # Select top 50 or all features if less\n",
    "    \n",
    "    # Method 1: Statistical tests\n",
    "    selector_stats = SelectKBest(score_func=f_classif, k=k)\n",
    "    X_stats = selector_stats.fit_transform(X, y)\n",
    "    stats_features = X.columns[selector_stats.get_support()].tolist()\n",
    "    \n",
    "    # Method 2: Mutual information\n",
    "    selector_mi = SelectKBest(score_func=mutual_info_classif, k=k)\n",
    "    X_mi = selector_mi.fit_transform(X, y)\n",
    "    mi_features = X.columns[selector_mi.get_support()].tolist()\n",
    "    \n",
    "    # Method 3: Random Forest importance\n",
    "    rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    rf_selector.fit(X, y)\n",
    "    importances = rf_selector.feature_importances_\n",
    "    top_rf_features = X.columns[np.argsort(importances)[-k:]].tolist()\n",
    "    \n",
    "    # Combine methods (union of top features)\n",
    "    combined_features = list(set(stats_features + mi_features + top_rf_features))\n",
    "    \n",
    "    print(f\"Feature selection: {len(combined_features)} features selected from {X.shape[1]}\")\n",
    "    return combined_features\n",
    "\n",
    "selected_features = intelligent_feature_selection(X_enhanced, y)\n",
    "X_selected = X_enhanced[selected_features]\n",
    "X_test_selected = X_test_enhanced[selected_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e9901",
   "metadata": {},
   "source": [
    "###  6. Train/Validation Split\n",
    "Splits the dataset into training and validation sets using stratified sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b09ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. TRAIN/VALIDATION SPLIT\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\n DATA SPLIT\")\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Training target distribution: {y_train.value_counts(normalize=True).round(3).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9bbb75",
   "metadata": {},
   "source": [
    "###  7. Optimized Preprocessing Pipeline\n",
    "Uses transformers like QuantileTransformer to preprocess numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488a442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. OPTIMIZED PREPROCESSING PIPELINE\n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"Create optimized preprocessing pipeline\"\"\"\n",
    "    \n",
    "    numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Use QuantileTransformer for better handling of skewed distributions\n",
    "    numeric_transformer = Pipeline([\n",
    "        ('scaler', QuantileTransformer(n_quantiles=100, random_state=42))\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ], remainder='passthrough')\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "preprocessor = create_preprocessing_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec92f585",
   "metadata": {},
   "source": [
    "###  8. Advanced Resampling Strategy\n",
    "Handles class imbalance using techniques like SMOTE, BorderlineSMOTE, and SMOTETomek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a4112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. ADVANCED RESAMPLING STRATEGY\n",
    "def get_optimal_resampling_strategy(X, y):\n",
    "    \"\"\"Determine optimal resampling based on class distribution\"\"\"\n",
    "    \n",
    "    class_counts = y.value_counts()\n",
    "    imbalance_ratio = class_counts[0] / class_counts[1] if class_counts[1] > 0 else float('inf')\n",
    "    \n",
    "    print(f\"Class imbalance ratio: {imbalance_ratio:.2f}\")\n",
    "    \n",
    "    if imbalance_ratio > 3:\n",
    "        # Severe imbalance - use SMOTE + undersampling\n",
    "        return SMOTETomek(sampling_strategy=0.7, random_state=42)\n",
    "    elif imbalance_ratio > 2:\n",
    "        # Moderate imbalance - use BorderlineSMOTE\n",
    "        return BorderlineSMOTE(sampling_strategy=0.8, random_state=42)\n",
    "    else:\n",
    "        # Mild imbalance - use standard SMOTE\n",
    "        return SMOTE(sampling_strategy=0.9, random_state=42)\n",
    "\n",
    "resampler = get_optimal_resampling_strategy(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e28f0",
   "metadata": {},
   "source": [
    "###  9. Optimized Model Configurations\n",
    "Defines pipelines for XGBoost, LightGBM, and RandomForest with tuned hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226e069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. OPTIMIZED MODEL CONFIGURATIONS\n",
    "def get_optimized_models():\n",
    "    \"\"\"Get carefully tuned models for high F1 performance\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    # XGBoost - optimized for F1 score\n",
    "    xgb_pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('resampler', resampler),\n",
    "        ('classifier', xgb.XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            eval_metric='logloss',\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    xgb_params = {\n",
    "        'classifier__n_estimators': [200, 300, 400],\n",
    "        'classifier__max_depth': [4, 6, 8],\n",
    "        'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "        'classifier__subsample': [0.8, 0.9],\n",
    "        'classifier__colsample_bytree': [0.8, 0.9],\n",
    "        'classifier__scale_pos_weight': [2, 3, 4, 5]\n",
    "    }\n",
    "    \n",
    "    models['xgb'] = (xgb_pipeline, xgb_params)\n",
    "    \n",
    "    # LightGBM - often excellent for tabular data\n",
    "    lgb_pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('resampler', resampler),\n",
    "        ('classifier', lgb.LGBMClassifier(\n",
    "            objective='binary',\n",
    "            metric='binary_logloss',\n",
    "            verbose=-1,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    lgb_params = {\n",
    "        'classifier__n_estimators': [200, 300, 400],\n",
    "        'classifier__max_depth': [4, 6, 8],\n",
    "        'classifier__learning_rate': [0.05, 0.1, 0.15],\n",
    "        'classifier__num_leaves': [31, 50, 70],\n",
    "        'classifier__class_weight': ['balanced']\n",
    "    }\n",
    "    \n",
    "    models['lgb'] = (lgb_pipeline, lgb_params)\n",
    "    \n",
    "    # Random Forest with balanced class weights\n",
    "    rf_pipeline = ImbPipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('resampler', resampler),\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    rf_params = {\n",
    "        'classifier__n_estimators': [300, 500],\n",
    "        'classifier__max_depth': [8, 12, 16],\n",
    "        'classifier__min_samples_split': [2, 5],\n",
    "        'classifier__min_samples_leaf': [1, 2],\n",
    "        'classifier__max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    models['rf'] = (rf_pipeline, rf_params)\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93e791",
   "metadata": {},
   "source": [
    "###  10. Efficient Hyperparameter Tuning\n",
    "Uses RandomizedSearchCV to efficiently find the best hyperparameters for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e12cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. EFFICIENT HYPERPARAMETER TUNING\n",
    "print(\"\\n HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "models = get_optimized_models()\n",
    "best_models = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, (pipeline, param_grid) in models.items():\n",
    "    print(f\"\\nTuning {name.upper()}...\")\n",
    "    \n",
    "    # Use RandomizedSearchCV for efficiency\n",
    "    search = RandomizedSearchCV(\n",
    "        pipeline, \n",
    "        param_grid,\n",
    "        n_iter=20,  # Reduced for efficiency\n",
    "        cv=cv,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        search.fit(X_train, y_train)\n",
    "        best_models[name] = search.best_estimator_\n",
    "        print(f\" {name} - Best F1: {search.best_score_:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" {name} failed: {str(e)[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462d0470",
   "metadata": {},
   "source": [
    "###  11. Advanced Threshold Optimization\n",
    "Finds the threshold that maximizes the F1 score on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd39500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. ADVANCED THRESHOLD OPTIMIZATION\n",
    "def optimize_threshold_advanced(model, X_val, y_val):\n",
    "    \"\"\"Advanced threshold optimization with F1 focus\"\"\"\n",
    "    \n",
    "    try:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            probs = model.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            probs = model.decision_function(X_val)\n",
    "            probs = (probs - probs.min()) / (probs.max() - probs.min())\n",
    "    except:\n",
    "        return 0.5, 0.0\n",
    "    \n",
    "    # Fine-grained threshold search\n",
    "    thresholds = np.linspace(0.1, 0.9, 81)\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (probs >= threshold).astype(int)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold, best_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03386020",
   "metadata": {},
   "source": [
    "###  12. Model Evaluation and Selection\n",
    "Evaluates each trained model and selects the best based on F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ac84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. MODEL EVALUATION AND SELECTION\n",
    "print(\"\\n MODEL EVALUATION\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for name, model in best_models.items():\n",
    "    try:\n",
    "        # Get predictions\n",
    "        y_pred_default = model.predict(X_val)\n",
    "        f1_default = f1_score(y_val, y_pred_default)\n",
    "        \n",
    "        # Optimize threshold\n",
    "        best_threshold, best_f1 = optimize_threshold_advanced(model, X_val, y_val)\n",
    "        \n",
    "        model_results[name] = {\n",
    "            'model': model,\n",
    "            'f1_default': f1_default,\n",
    "            'f1_optimized': best_f1,\n",
    "            'threshold': best_threshold\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{name.upper()}:\")\n",
    "        print(f\"  F1 (default): {f1_default:.4f}\")\n",
    "        print(f\"  F1 (optimized): {best_f1:.4f}\")\n",
    "        print(f\"  Best threshold: {best_threshold:.3f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Error evaluating {name}: {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0521e",
   "metadata": {},
   "source": [
    "###  13. Create Ensemble if Beneficial\n",
    "Creates a soft voting ensemble using the top 2-3 performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26d8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. CREATE ENSEMBLE IF BENEFICIAL\n",
    "if len(model_results) >= 2:\n",
    "    print(\"\\n ENSEMBLE CREATION\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    # Sort models by F1 score\n",
    "    sorted_models = sorted(model_results.items(), \n",
    "                          key=lambda x: x[1]['f1_optimized'], \n",
    "                          reverse=True)\n",
    "    \n",
    "    # Take top 2-3 models for ensemble\n",
    "    top_models = [(name, result['model']) for name, result in sorted_models[:3]]\n",
    "    \n",
    "    # Create voting ensemble\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=top_models,\n",
    "        voting='soft'\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        ensemble.fit(X_train, y_train)\n",
    "        ensemble_threshold, ensemble_f1 = optimize_threshold_advanced(ensemble, X_val, y_val)\n",
    "        \n",
    "        model_results['ensemble'] = {\n",
    "            'model': ensemble,\n",
    "            'f1_optimized': ensemble_f1,\n",
    "            'threshold': ensemble_threshold\n",
    "        }\n",
    "        \n",
    "        print(f\"Ensemble F1: {ensemble_f1:.4f} (threshold: {ensemble_threshold:.3f})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ensemble creation failed: {str(e)[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e73a9",
   "metadata": {},
   "source": [
    "###  14. Final Model Selection and Prediction\n",
    "Retrains the best model on full data and generates predictions for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661d29ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. FINAL MODEL SELECTION AND PREDICTION\n",
    "print(\"\\n FINAL MODEL SELECTION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "if model_results:\n",
    "    # Select best model\n",
    "    best_model_name = max(model_results.keys(), \n",
    "                         key=lambda k: model_results[k]['f1_optimized'])\n",
    "    \n",
    "    final_model = model_results[best_model_name]['model']\n",
    "    final_threshold = model_results[best_model_name]['threshold']\n",
    "    final_f1 = model_results[best_model_name]['f1_optimized']\n",
    "    \n",
    "    print(f\"Selected: {best_model_name.upper()}\")\n",
    "    print(f\"Expected F1: {final_f1:.4f}\")\n",
    "    print(f\"Threshold: {final_threshold:.3f}\")\n",
    "    \n",
    "    # Retrain on full dataset\n",
    "    print(\"\\n Retraining on full dataset...\")\n",
    "    final_model.fit(X_selected, y)\n",
    "    \n",
    "    # Generate final predictions\n",
    "    print(\" Generating final predictions...\")\n",
    "    \n",
    "    try:\n",
    "        if hasattr(final_model, 'predict_proba'):\n",
    "            test_probs = final_model.predict_proba(X_test_selected)[:, 1]\n",
    "            final_predictions = (test_probs >= final_threshold).astype(int)\n",
    "        else:\n",
    "            final_predictions = final_model.predict(X_test_selected)\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction error: {e}\")\n",
    "        final_predictions = final_model.predict(X_test_selected)\n",
    "    \n",
    "    # Create submission\n",
    "    submission = pd.DataFrame({\n",
    "        'age_group': final_predictions\n",
    "    })\n",
    "    \n",
    "    submission.to_csv('optimized_submission.csv', index=False)\n",
    "    \n",
    "    print(f\"\\n SUBMISSION READY!\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"File: optimized_submission.csv\")\n",
    "    print(f\"Total predictions: {len(final_predictions)}\")\n",
    "    \n",
    "    pred_counts = pd.Series(final_predictions).value_counts()\n",
    "    print(f\"Adult (0): {pred_counts.get(0, 0)} ({pred_counts.get(0, 0)/len(final_predictions)*100:.1f}%)\")\n",
    "    print(f\"Senior (1): {pred_counts.get(1, 0)} ({pred_counts.get(1, 0)/len(final_predictions)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nModel: {best_model_name.upper()}\")\n",
    "    print(f\"Expected F1 Score: {final_f1:.4f}\")\n",
    "    \n",
    "    if final_f1 > 0.6:\n",
    "        print(\" EXCELLENT! This should achieve 50+ F1 score!\")\n",
    "    elif final_f1 > 0.5:\n",
    "        print(\" GOOD performance expected!\")\n",
    "    else:\n",
    "        print(\" May need further optimization\")\n",
    "\n",
    "else:\n",
    "    print(\" No models were successfully trained\")\n",
    "\n",
    "print(\"\\n PROCESS COMPLETE!\")\n",
    "print(\"=\" * 40)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
